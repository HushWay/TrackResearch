import os
import json
import time
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from xml.etree import ElementTree as ET

from Bio import Entrez
from openai import OpenAI

# ========== ç”¨æˆ·é…ç½®éƒ¨åˆ† ==========
# ä»¥ä¸‹éƒ¨åˆ†ä¸ºå¯é…ç½®é¡¹ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„ç ”ç©¶æ–¹å‘è¿›è¡Œä¿®æ”¹

# ç ”ç©¶æ–¹å‘åç§°ï¼ˆç”¨äºGitHub Issueæ ‡é¢˜ï¼‰
RESEARCH_AREA = "è‚¿ç˜¤åŸºå› ç»„å…‹éš†ç»“æ„å’Œç³»ç»Ÿå‘ç”Ÿæ ‘åˆ†æ"

# APIå¯†é’¥é…ç½® - é€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥
ENTREZ_EMAIL = "shehuizhuyitese@gmail.com"  # å¿…å¡«
ENTREZ_API_KEY = os.getenv("NCBI_API_KEY")  # å¯é€‰ï¼Œä½†å»ºè®®é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # å¦‚æœä½¿ç”¨OpenAI
AIZEX_API_KEY = os.getenv("AIZEX_API_KEY")    # å¦‚æœä½¿ç”¨AIZEX
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")      # å¿…å¡«ï¼Œç”¨äºåˆ›å»ºIssue
GITHUB_REPO_OWNER = "HushWay"      # GitHubä»“åº“æ‰€æœ‰è€…
GITHUB_REPO_NAME = "TrackResearch"             # GitHubä»“åº“åç§°

# APIæœåŠ¡é…ç½®
API_BASE_URL = "https://a1.aizex.me/v1"  # å¯ä»¥æ›¿æ¢ä¸ºOpenAIæˆ–å…¶ä»–å…¼å®¹æœåŠ¡
AI_MODEL = "gpt-4o-mini"  # ä½¿ç”¨çš„æ¨¡å‹åç§°

# PubMed/PMCæ£€ç´¢å…³é”®è¯é…ç½® - æ ¹æ®ç ”ç©¶æ–¹å‘ä¿®æ”¹
TIAB_TERMS = [
    "clonal structure[tiab]", "clone structure[tiab]", 
    "phylogenetic tree[tiab]", "clonal phylogeny[tiab]",
    "clonal evolution[tiab]", "clonal architecture[tiab]",
    "clonal reconstruction[tiab]", "subclonal reconstruction[tiab]",
    "tumor phylogeny[tiab]", "cancer phylogeny[tiab]",
    "subclonal architecture[tiab]", "phylogenetic analysis[tiab]",
    "lineage tracing[tiab]", "cancer lineage[tiab]",
    "tumor lineage[tiab]", "clonality analysis[tiab]"
]

MESH_TERMS = [
    "Phylogeny[MeSH Terms]", 
    "Genetic Heterogeneity[MeSH Terms]",
    "Neoplasms/genetics[MeSH Terms]",
    "Genomics/methods[MeSH Terms]",
    "Computational Biology/methods[MeSH Terms]"
]

# é«˜å½±å“åŠ›æ‚å¿—åˆ—è¡¨ - æ ¹æ®ç ”ç©¶é¢†åŸŸä¿®æ”¹
HIGH_IMPACT_JOURNALS = [
    "Nature", "Science", "Cell", "Nature Genetics", 
    "Nature Methods", "Nature Biotechnology", "Cancer Cell", 
    "Nature Communications", "Science Advances", 
    "Genome Biology", "Genome Research", "Cancer Discovery",
    "Nature Reviews Cancer", "Cell Reports", "PNAS", 
    "Molecular Cell", "Bioinformatics", "Nucleic Acids Research"
]

# é«˜å½±å“åŠ›æ‚å¿—åŠ åˆ†
JOURNAL_IMPACT_BONUS = 10  # é«˜å½±å“åŠ›æ‚å¿—æ–‡ç« çš„é¢å¤–åˆ†æ•°

# é¢„è¿‡æ»¤ç›¸å…³åº¦å…³é”®è¯ - æ ¹æ®ç ”ç©¶æ–¹å‘ä¿®æ”¹
HIGH_RELEVANCE_KEYWORDS = [
    'clonal structure', 'phylogenetic tree', 'clonal phylogeny',
    'tumor phylogeny', 'cancer phylogeny', 'phylogenetic analysis',
    'clone structure', 'clonal architecture', 'subclonal architecture',
    'clonal reconstruction', 'subclonal reconstruction',
    'clone evolution', 'tumor lineage', 'cancer lineage',
    'lineage tracing', 'clonality analysis', 'ancestral clone',
    'clonal dynamics', 'mutation tree', 'evolutionary tree',
    'phylogenetic inference', 'clonal inference', 'treeomics',
    'clonevol', 'clone tracking', 'clonal complexity',
    'subclone mapping', 'clonal tracking'
]

# é¢„è¿‡æ»¤åŒ¹é…é˜ˆå€¼
KEYWORD_MATCH_THRESHOLD = 2  # åŒ¹é…å¤šå°‘ä¸ªå…³é”®è¯æ‰ä¿ç•™

# LLMè¯„åˆ†æç¤ºè¯æ¨¡æ¿ - æ ¹æ®ç ”ç©¶æ–¹å‘ä¿®æ”¹
SCORING_SYSTEM_PROMPT = "ä½ æ˜¯{research_area}ä¸“å®¶ã€‚ä½ éœ€è¦è¯†åˆ«ä¸{research_area}ç›¸å…³çš„ç ”ç©¶ã€‚"

SCORING_USER_PROMPT = """å¯¹ä¸‹é¢çš„æ–‡ç« è¿›è¡Œ0-100åˆ†æ‰“åˆ†ï¼Œè¯„ä¼°å…¶ä¸{research_area}çš„ç›¸å…³æ€§ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- 90-100ï¼šç›´æ¥ç ”ç©¶{research_area}çš„æ ¸å¿ƒæ–‡ç« 
- 70-89ï¼šä¸{research_area}æœ‰å¯†åˆ‡å…³ç³»çš„æ–¹æ³•å­¦æ–‡ç« 
- 40-69ï¼šæåˆ°{research_area}ä½†ä¸»è¦ç ”ç©¶å…¶ä»–æ–¹é¢çš„æ–‡ç« 
- 0-39ï¼šåŸºæœ¬ä¸ç›¸å…³çš„æ–‡ç« 

æ ‡é¢˜: {title}
æ‘˜è¦: {abstract}

åªè¿”å›åˆ†æ•°æ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚"""

# æ‰¹é‡è¯„åˆ†æç¤ºè¯æ¨¡æ¿
BATCH_SCORING_USER_PROMPT = """å¯¹ä»¥ä¸‹å¤šç¯‡æ–‡ç« è¿›è¡Œæ‰“åˆ†ï¼ˆ0-100ï¼‰ï¼Œè¯„ä¼°å®ƒä»¬ä¸{research_area}çš„ç›¸å…³æ€§ï¼š

è¯„åˆ†æ ‡å‡†ï¼š
- 90-100åˆ†ï¼šç›´æ¥ç ”ç©¶{research_area}çš„æ ¸å¿ƒæ–‡ç« 
- 70-89åˆ†ï¼šä¸{research_area}æœ‰å¯†åˆ‡å…³ç³»çš„æ–¹æ³•å­¦æ–‡ç« 
- 40-69åˆ†ï¼šæåˆ°{research_area}ä½†ä¸»è¦ç ”ç©¶å…¶ä»–æ–¹é¢çš„æ–‡ç« 
- 0-39åˆ†ï¼šåŸºæœ¬ä¸ç›¸å…³çš„æ–‡ç« 

{articles}

è¯·ç”¨JSONæ ¼å¼è¿”å›ç»“æœï¼Œé”®ä¸ºæ–‡ç« IDï¼Œå€¼ä¸ºåˆ†æ•°ï¼Œä¾‹å¦‚ï¼š
{{"article_0": 85, "article_1": 45, ...}}"""

# æ£€ç´¢é…ç½®
DEEP_SEARCH_MONTHS = 12     # æ·±åº¦æ£€ç´¢çš„æ—¶é—´èŒƒå›´ï¼ˆæœˆï¼‰
DEEP_SEARCH_RETMAX = 100    # æ·±åº¦æ£€ç´¢çš„æœ€å¤§æ–‡ç« æ•°
RECENT_SEARCH_DAYS = 7      # æœ€è¿‘æ£€ç´¢çš„æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
RECENT_SEARCH_RETMAX = 50   # æœ€è¿‘æ£€ç´¢çš„æœ€å¤§æ–‡ç« æ•°
ARXIV_MAX_RESULTS = 50      # arXivæ£€ç´¢çš„æœ€å¤§æ–‡ç« æ•°

# ç¼“å­˜æ–‡ä»¶é…ç½®
CACHE_FILE = "article_cache.json"  # ç¼“å­˜æ–‡ä»¶è·¯å¾„

# æ—¥å¿—é…ç½®
LOG_FILE = "research_tracker.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
LOG_LEVEL = "INFO"                 # æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)

# LLM APIé…ç½®
BATCH_SIZE = 5              # æ‰¹é‡è¯„åˆ†çš„æ‰¹æ¬¡å¤§å°
MAX_RETRIES = 3             # APIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
BACKOFF_FACTOR = 2          # é‡è¯•é—´éš”é€’å¢å› å­

# ========== åˆå§‹åŒ–éƒ¨åˆ† ==========

# åˆå§‹åŒ–APIå®¢æˆ·ç«¯
client = OpenAI(
    api_key=AIZEX_API_KEY or OPENAI_API_KEY,
    base_url=API_BASE_URL
)

# è®¾ç½®Entrezé‚®ç®±å’ŒAPIå¯†é’¥
Entrez.email = ENTREZ_EMAIL
if ENTREZ_API_KEY:
    Entrez.api_key = ENTREZ_API_KEY

# è®¾ç½®æ—¥å¿—
import logging
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ========== åŠŸèƒ½å®ç°éƒ¨åˆ† ==========

def build_pubmed_query():
    """æ„å»ºPubMedæŸ¥è¯¢è¯­å¥"""
    tiab = " OR ".join(TIAB_TERMS)
    mesh = " OR ".join(MESH_TERMS)
    return f"({tiab}) OR ({mesh})"

# === ç¼“å­˜ç®¡ç† ===
def normalize_string(s):
    """ç®€å•çš„å­—ç¬¦ä¸²è§„èŒƒåŒ–ï¼Œä¾¿äºæ¯”è¾ƒ"""
    if not s:
        return ""
    return "".join(s.lower().split())

def load_article_cache(cache_file=CACHE_FILE):
    """åŠ è½½æ–‡ç« ç¼“å­˜"""
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache = json.load(f)
            logger.info(f"å·²åŠ è½½ {len(cache.get('articles', []))} ç¯‡ç¼“å­˜æ–‡ç« ")
            return cache
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
    return {"last_update": "", "articles": []}

def save_article_cache(cache, cache_file=CACHE_FILE):
    """ä¿å­˜æ–‡ç« ç¼“å­˜"""
    cache["last_update"] = datetime.now().isoformat()
    try:
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        logger.info(f"å·²ä¿å­˜ {len(cache.get('articles', []))} ç¯‡æ–‡ç« åˆ°ç¼“å­˜")
    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def merge_with_cache(new_articles, cache_file=CACHE_FILE):
    """å°†æ–°æŠ“å–çš„æ–‡ç« ä¸ç¼“å­˜åˆå¹¶ï¼Œé¿å…é‡å¤è¯·æ±‚"""
    cache = load_article_cache(cache_file)
    cached_articles = cache.get("articles", [])
    
    # æ„å»ºæ ‡é¢˜å’ŒDOIç´¢å¼•ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
    cached_titles = {normalize_string(art.get("title", "")): art for art in cached_articles if art.get("title")}
    cached_dois = {art.get("doi", ""): art for art in cached_articles if art.get("doi")}
    
    merged_articles = []
    new_count = 0
    
    for article in new_articles:
        title = normalize_string(article.get("title", ""))
        doi = article.get("doi", "")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç¼“å­˜ä¸­
        cached_art = None
        if doi and doi in cached_dois:
            cached_art = cached_dois[doi]
        elif title and title in cached_titles:
            cached_art = cached_titles[title]
            
        if cached_art and "score" in cached_art:
            # ä½¿ç”¨ç¼“å­˜çš„åˆ†æ•°
            article["score"] = cached_art["score"]
            merged_articles.append(article)
        else:
            # æ–°æ–‡ç« ï¼Œéœ€è¦æ‰“åˆ†
            merged_articles.append(article)
            new_count += 1
    
    logger.info(f"åˆå¹¶åå…± {len(merged_articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {new_count} ç¯‡æ–°æ–‡ç« éœ€è¦æ‰“åˆ†")
    return merged_articles, new_count

def update_cache_with_scores(articles, cache_file=CACHE_FILE):
    """æ›´æ–°ç¼“å­˜ï¼Œæ·»åŠ æ–°æ‰“åˆ†çš„æ–‡ç« """
    cache = load_article_cache(cache_file)
    cached_articles = cache.get("articles", [])
    
    # æ„å»ºæ ‡é¢˜å’ŒDOIç´¢å¼•
    cached_titles = {normalize_string(art.get("title", "")): i for i, art in enumerate(cached_articles) if art.get("title")}
    cached_dois = {art.get("doi", ""): i for i, art in enumerate(cached_articles) if art.get("doi")}
    
    # æ›´æ–°ç°æœ‰æ–‡ç« æˆ–æ·»åŠ æ–°æ–‡ç« 
    for article in articles:
        if "score" not in article:
            continue  # è·³è¿‡æœªæ‰“åˆ†çš„æ–‡ç« 
            
        title = normalize_string(article.get("title", ""))
        doi = article.get("doi", "")
        
        if doi and doi in cached_dois:
            # æ›´æ–°å·²æœ‰æ–‡ç« 
            cached_articles[cached_dois[doi]]["score"] = article["score"]
        elif title and title in cached_titles:
            # æ›´æ–°å·²æœ‰æ–‡ç« 
            cached_articles[cached_titles[title]]["score"] = article["score"]
        else:
            # æ·»åŠ æ–°æ–‡ç« 
            cached_articles.append(article)
    
    cache["articles"] = cached_articles
    save_article_cache(cache, cache_file)
    return cache

# === æ–‡çŒ®æŠ“å– ===
def fetch_pubmed(months_back=DEEP_SEARCH_MONTHS, retmax=DEEP_SEARCH_RETMAX):
    """æ·±åº¦æ£€ç´¢PubMedï¼Œè·å–è¿‡å»å‡ ä¸ªæœˆçš„ç›¸å…³æ–‡çŒ®"""
    logger.info(f"å¼€å§‹æ·±åº¦æ£€ç´¢PubMed, æ—¶é—´èŒƒå›´: {months_back}ä¸ªæœˆ, æœ€å¤§æ•°é‡: {retmax}")
    try:
        end = datetime.utcnow()
        start = end - timedelta(days=30*months_back)
        query = build_pubmed_query()
        handle = Entrez.esearch(
            db="pubmed",
            term=query,
            retmax=retmax,
            datetype="pdat",
            mindate=start.strftime("%Y/%m/%d"),
            maxdate=end.strftime("%Y/%m/%d")
        )
        record = Entrez.read(handle)
        pmids = record["IdList"]
        if not pmids:
            logger.warning("æ·±åº¦æ£€ç´¢æœªæ‰¾åˆ°ç»“æœ")
            return []

        results = fetch_pubmed_from_pmids(pmids)
        logger.info(f"æ·±åº¦æ£€ç´¢å®Œæˆï¼Œè·å–äº† {len(results)} ç¯‡æ–‡ç« ")
        return results
    except Exception as e:
        logger.error(f"æ·±åº¦æ£€ç´¢å‡ºé”™: {str(e)}")
        return []

def fetch_pubmed_from_pmids(pmids):
    """æ ¹æ®PMIDåˆ—è¡¨è·å–æ–‡ç« è¯¦æƒ…"""
    if not pmids:
        return []
    try:
        fetch = Entrez.efetch(
            db="pubmed",
            id=",".join(pmids),
            rettype="xml"
        )
        root = ET.fromstring(fetch.read())
        results = []
        for article in root.findall(".//PubmedArticle"):
            art = {}
            art["title"] = article.findtext(".//ArticleTitle") or ""
            art["abstract"] = "".join([t.text or "" for t in article.findall(".//AbstractText")])
            art["doi"] = article.findtext(".//ArticleId[@IdType='doi']") or ""
            pmid = article.findtext(".//PMID")
            art["link"] = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else ""
            art["journal"] = (
                article.findtext(".//Journal/Title")
                or article.findtext(".//MedlineJournalInfo/JournalTitle")
                or ""
            )
            # æå–å®Œæ•´æ—¥æœŸ
            pubdate = article.find(".//PubDate")
            if pubdate is not None:
                year  = pubdate.findtext("Year") or ""
                month = pubdate.findtext("Month") or ""
                day   = pubdate.findtext("Day") or ""
                dt = None
                try:
                    dt = datetime.strptime(f"{year} {month} {day}", "%Y %b %d")
                except:
                    try:
                        dt = datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
                    except:
                        dt = None
                art["pub_date"] = dt.strftime("%Y-%m-%d") if dt else year
            else:
                art["pub_date"] = ""
            results.append(art)
        return results
    except Exception as e:
        logger.error(f"ä»PMIDè·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")
        return []

def fetch_trending_pubmed(days=RECENT_SEARCH_DAYS, retmax=RECENT_SEARCH_RETMAX):
    """è·å–æœ€è¿‘å‡ å¤©çš„PubMedæ–‡ç« """
    logger.info(f"å¼€å§‹å¯»æ–°æ£€ç´¢PubMed, æ—¶é—´èŒƒå›´: {days}å¤©, æœ€å¤§æ•°é‡: {retmax}")
    try:
        end = datetime.utcnow()
        start = end - timedelta(days=days)
        query = build_pubmed_query()
        handle = Entrez.esearch(
            db="pubmed",
            term=query,
            retmax=retmax,
            datetype="pdat",
            mindate=start.strftime("%Y/%m/%d"),
            maxdate=end.strftime("%Y/%m/%d")
        )
        record = Entrez.read(handle)
        pmids = record["IdList"]
        results = fetch_pubmed_from_pmids(pmids)
        logger.info(f"å¯»æ–°æ£€ç´¢å®Œæˆï¼Œè·å–äº† {len(results)} ç¯‡æ–‡ç« ")
        return results
    except Exception as e:
        logger.error(f"å¯»æ–°æ£€ç´¢å‡ºé”™: {str(e)}")
        return []

def fetch_europe_pmc(days=RECENT_SEARCH_DAYS, pageSize=RECENT_SEARCH_RETMAX):
    """è·å–Europe PMCæœ€è¿‘æ–‡ç« """
    logger.info(f"å¼€å§‹æ£€ç´¢Europe PMC, æ—¶é—´èŒƒå›´: {days}å¤©, æœ€å¤§æ•°é‡: {pageSize}")
    try:
        end = datetime.utcnow().strftime("%Y-%m-%d")
        start = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%d")
        query = build_pubmed_query()
        url = (
            "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
            f"?query={requests.utils.quote(query)}"
            f"+PUB_TYPE:preprint+OR+PUB_TYPE:journal"
            f"&dateFilter=PUB_CITED_TS:[{start}%20TO%20{end}]"
            "&format=json"
            f"&pageSize={pageSize}"
        )
        resp = requests.get(url, timeout=30)
        data = resp.json().get("resultList", {}).get("result", [])
        results = []
        for item in data:
            art = {
                "title": item.get("title", ""),
                "abstract": item.get("abstractText", ""),
                "doi": item.get("doi", ""),
                "link": item.get("fullTextUrlList", {}).get("fullTextUrl", [{}])[0].get("url", ""),
                # Europe PMC æœ¬èº«å·²æä¾› YYYY-MM-DD æ ¼å¼
                "pub_date": item.get("firstPublicationDate", ""),
                "journal": item.get("journalTitle", "")
            }
            results.append(art)
        logger.info(f"Europe PMCæ£€ç´¢å®Œæˆï¼Œè·å–äº† {len(results)} ç¯‡æ–‡ç« ")
        return results
    except Exception as e:
        logger.error(f"Europe PMCæ£€ç´¢å‡ºé”™: {str(e)}")
        return []

def fetch_arxiv(max_results=ARXIV_MAX_RESULTS):
    """è·å–arXivç›¸å…³é¢„å°æœ¬"""
    logger.info(f"å¼€å§‹æ£€ç´¢ArXiv, æœ€å¤§æ•°é‡: {max_results}")
    try:
        # å…ˆè·å–å…³é”®è¯åˆ—è¡¨ï¼Œå»æ‰[tiab]æ ‡è®°
        terms = [t.split("[")[0] for t in TIAB_TERMS]
        
        # ä½¿ç”¨ requests.utils.quote å¯¹æ•´ä¸ªæŸ¥è¯¢è¿›è¡Œç¼–ç 
        raw_query = "cat:q-bio*+AND+(" + "+OR+".join([requests.utils.quote(term) for term in terms]) + ")"
        
        url = (
            "http://export.arxiv.org/api/query"
            f"?search_query={raw_query}"
            f"&start=0&max_results={max_results}"
            "&sortBy=submittedDate&sortOrder=descending"
        )
        feed = feedparser.parse(url)
        results = []
        for entry in feed.entries:
            journal_ref = entry.get('arxiv_journal_ref', '')
            art = {
                "title": entry.title,
                "abstract": entry.summary,
                "doi": next((l.href for l in entry.links if l.href.startswith("http://dx.doi.org/")), ""),
                "link": entry.link,
                "pub_date": entry.published[:10],
                "journal": journal_ref or "arXiv"
            }
            results.append(art)
        logger.info(f"ArXivæ£€ç´¢å®Œæˆï¼Œè·å–äº† {len(results)} ç¯‡æ–‡ç« ")
        return results
    except Exception as e:
        logger.error(f"ArXivæ£€ç´¢å‡ºé”™: {str(e)}")
        return []

# === é¢„è¿‡æ»¤ ===
def simple_keyword_filter(articles, threshold=KEYWORD_MATCH_THRESHOLD):
    """ä½¿ç”¨å…³é”®è¯åŒ¹é…è¿›è¡Œé¢„è¿‡æ»¤ï¼Œå‡å°‘éœ€è¦APIè¯„åˆ†çš„æ–‡ç« æ•°é‡"""
    filtered_articles = []
    for art in articles:
        text = (art.get('title', '') + ' ' + art.get('abstract', '')).lower()
        # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
        matches = sum(1 for keyword in HIGH_RELEVANCE_KEYWORDS if keyword.lower() in text)
        if matches >= threshold:
            filtered_articles.append(art)
    
    logger.info(f"é¢„è¿‡æ»¤: ä» {len(articles)} ç¯‡æ–‡ç« ä¸­ç­›é€‰å‡º {len(filtered_articles)} ç¯‡")
    return filtered_articles

# === è¯„åˆ† & ç­›é€‰ ===
def extract_relevance_score(article, max_retries=MAX_RETRIES, backoff_factor=BACKOFF_FACTOR):
    """å•ç¯‡æ–‡ç« è¯„åˆ†ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶"""
    text = article.get("abstract", "")
    title = article.get("title", "")
    
    for attempt in range(max_retries):
        try:
            system_prompt = SCORING_SYSTEM_PROMPT.format(research_area=RESEARCH_AREA)
            user_prompt = SCORING_USER_PROMPT.format(
                research_area=RESEARCH_AREA,
                title=title,
                abstract=text
            )
            
            response = client.chat.completions.create(
                model=AI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=5,
                temperature=0
            )
            txt = response.choices[0].message.content.strip()
            try:
                score = min(100, int(''.join(filter(str.isdigit, txt))))
                article["score"] = score
                return score
            except:
                article["score"] = 0
                return 0
        except Exception as e:
            logger.warning(f"è¯„åˆ†å°è¯• {attempt+1}/{max_retries} å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                sleep_time = backoff_factor ** attempt
                logger.info(f"ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                time.sleep(sleep_time)
            else:
                logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›é»˜è®¤åˆ†æ•°")
                article["score"] = 30
                return 30  # è¿”å›é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    article["score"] = 30
    return 30  # ä»¥é˜²ä¸‡ä¸€ï¼Œç¡®ä¿è¿”å›ä¸€ä¸ªé»˜è®¤å€¼

def batch_extract_relevance_scores(articles, batch_size=BATCH_SIZE, max_retries=MAX_RETRIES, backoff_factor=BACKOFF_FACTOR):
    """æ‰¹é‡å¤„ç†æ–‡ç« è¯„åˆ†ï¼Œæ¯æ¬¡APIè°ƒç”¨å¤„ç†å¤šç¯‡æ–‡ç« """
    all_scores = {}
    
    # å°†æ–‡ç« åˆ†æˆæ‰¹æ¬¡
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i+batch_size]
        batch_ids = [f"article_{j}" for j in range(i, i+len(batch))]
        
        # æ„å»ºæ‰¹é‡è¯·æ±‚çš„æ–‡ç« å†…å®¹
        articles_content = ""
        for idx, art in zip(batch_ids, batch):
            articles_content += f"--- {idx} ---\n"
            articles_content += f"æ ‡é¢˜: {art.get('title', '')}\n"
            # æ‘˜è¦å¯èƒ½è¿‡é•¿ï¼Œå–å‰1000å­—ç¬¦
            abstract = art.get('abstract', '')
            articles_content += f"æ‘˜è¦: {abstract[:1000]}{'...' if len(abstract) > 1000 else ''}\n\n"
        
        # æ„å»ºæ‰¹é‡è¯·æ±‚çš„å®Œæ•´æç¤º
        system_prompt = SCORING_SYSTEM_PROMPT.format(research_area=RESEARCH_AREA)
        user_prompt = BATCH_SCORING_USER_PROMPT.format(
            research_area=RESEARCH_AREA,
            articles=articles_content
        )
        
        # æ·»åŠ é‡è¯•é€»è¾‘
        for attempt in range(max_retries):
            try:
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(articles)-1)//batch_size + 1}")
                response = client.chat.completions.create(
                    model=AI_MODEL,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=500,  # å¢åŠ tokenä»¥å®¹çº³å¤šç¯‡æ–‡ç« çš„è¯„åˆ†ç»“æœ
                    temperature=0
                )
                
                # è§£æç»“æœ
                response_text = response.choices[0].message.content.strip()
                try:
                    # æå–JSONéƒ¨åˆ† (å¯èƒ½ä¼šæœ‰å…¶ä»–æ–‡æœ¬)
                    import re
                    json_str = re.search(r'{.*}', response_text, re.DOTALL)
                    if json_str:
                        import json
                        scores_dict = json.loads(json_str.group(0))
                        
                        # æ›´æ–°æ€»åˆ†æ•°å­—å…¸
                        for j, art_id in enumerate(batch_ids):
                            if art_id in scores_dict:
                                article_index = i + j
                                if article_index < len(articles):
                                    all_scores[article_index] = min(100, int(scores_dict[art_id]))
                            else:
                                logger.warning(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ç« ID {art_id}çš„åˆ†æ•°")
                        
                        # æˆåŠŸå¤„ç†ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        break
                    else:
                        raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼æ•°æ®")
                except Exception as e:
                    logger.error(f"è§£ææ‰¹æ¬¡ç»“æœå¤±è´¥: {e}")
                    logger.debug(f"åŸå§‹å“åº”: {response_text}")
                    if attempt < max_retries - 1:
                        continue
                    else:  # æœ€åä¸€æ¬¡å°è¯•ï¼Œä¸ºæ‰¹æ¬¡ä¸­æ–‡ç« åˆ†é…é»˜è®¤åˆ†æ•°
                        for j in range(len(batch)):
                            article_index = i + j
                            if article_index < len(articles):
                                all_scores[article_index] = 30  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
            
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {i//batch_size + 1} å°è¯• {attempt+1}/{max_retries} å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    sleep_time = backoff_factor ** attempt
                    logger.info(f"ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
                else:
                    logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸ºæ‰¹æ¬¡ä¸­æ‰€æœ‰æ–‡ç« åˆ†é…é»˜è®¤åˆ†æ•°")
                    for j in range(len(batch)):
                        article_index = i + j
                        if article_index < len(articles):
                            all_scores[article_index] = 30
        
        # åœ¨æ‰¹æ¬¡ä¹‹é—´ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œé¿å…APIé€Ÿç‡é™åˆ¶
        time.sleep(1)
    
    # å°†åˆ†æ•°åº”ç”¨åˆ°æ–‡ç« ä¸Š
    for i, art in enumerate(articles):
        art["score"] = all_scores.get(i, 30)  # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    
    return articles

def journal_impact_bonus(article):
    """ä¸ºé«˜å½±å“åŠ›æ‚å¿—çš„æ–‡ç« å¢åŠ é¢å¤–åˆ†æ•°"""
    journal = article.get("journal", "").lower()
    
    # ç²¾ç¡®åŒ¹é…é«˜å½±å“åŠ›æ‚å¿—
    for high_impact in HIGH_IMPACT_JOURNALS:
        if high_impact.lower() in journal:
            return JOURNAL_IMPACT_BONUS
    
    return 0

# === GitHub Issue ===
def create_github_issue(title, body):
    """åˆ›å»ºGitHub Issueæ¥å±•ç¤ºç»“æœ"""
    try:
        url = f"https://api.github.com/repos/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/issues"
        headers = {
            "Authorization": f"token {GITHUB_TOKEN}",
            "Accept": "application/vnd.github.v3+json"
        }
        data = {"title": title, "body": body}
        r = requests.post(url, headers=headers, json=data, timeout=30)
        if r.status_code == 201:
            logger.info(f"æˆåŠŸåˆ›å»ºIssue: {title}")
            return True
        else:
            logger.error(f"åˆ›å»ºIssueå¤±è´¥ï¼ŒçŠ¶æ€ç : {r.status_code}, å“åº”: {r.text}")
            return False
    except Exception as e:
        logger.error(f"åˆ›å»ºIssueè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        return False

# ========== ä¸»æµç¨‹ ==========
def main():
    try:
        # 1) æ·±åº¦æ£€ç´¢è¿‡å»å‡ ä¸ªæœˆç»å…¸æ–‡çŒ®
        deep = fetch_pubmed(months_back=DEEP_SEARCH_MONTHS, retmax=DEEP_SEARCH_RETMAX)
        
        # 2) å¯»æ–°æ£€ç´¢è¿‡å»å‡ å¤©æœ€æ–°è¿›å±•ï¼ˆPubMed + Europe PMC + arXivï¼‰
        new_pm = fetch_trending_pubmed(days=RECENT_SEARCH_DAYS, retmax=RECENT_SEARCH_RETMAX)
        new_ep = fetch_europe_pmc(days=RECENT_SEARCH_DAYS, pageSize=RECENT_SEARCH_RETMAX)
        new_ax = fetch_arxiv(max_results=ARXIV_MAX_RESULTS)

        all_articles = deep + new_pm + new_ep + new_ax
        logger.info(f"æ€»æŠ“å–æ–‡ç« æ•°: {len(all_articles)}")

        # 3) å»é‡
        unique_articles = []
        seen_titles = set()
        for art in all_articles:
            norm = normalize_string(art.get("title", ""))
            if norm and norm not in seen_titles:
                seen_titles.add(norm)
                unique_articles.append(art)
        logger.info(f"å»é‡åæ–‡ç« æ•°: {len(unique_articles)}")

        # 4) ä¸ç¼“å­˜åˆå¹¶
        merged_articles, new_count = merge_with_cache(unique_articles)

        # 5) å¯¹æ–°æ–‡ç« è¯„åˆ†
        if new_count > 0:
            new_articles = [a for a in merged_articles if "score" not in a]
            # é¢„è¿‡æ»¤å‡å°‘éœ€è¦è¯„åˆ†çš„æ–‡ç« æ•°é‡
            filtered = simple_keyword_filter(new_articles, threshold=KEYWORD_MATCH_THRESHOLD)
            # å¦‚æœè¿‡æ»¤åæ–‡ç« å¤ªå°‘ï¼Œé™ä½é˜ˆå€¼
            if len(filtered) < 10:
                filtered = simple_keyword_filter(new_articles, threshold=1)
            # å¦‚æœè¿˜æ˜¯å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰æ–°æ–‡ç« 
            if len(filtered) < 10:
                filtered = new_articles
                
            logger.info(f"éœ€è¦æ‰“åˆ†çš„æ–°æ–‡ç« : {len(filtered)}")
            
            if filtered:
                # æ‰¹é‡è¯„åˆ†
                batch_extract_relevance_scores(filtered, batch_size=BATCH_SIZE, 
                                              max_retries=MAX_RETRIES, 
                                              backoff_factor=BACKOFF_FACTOR)
                                              
                # å°†åˆ†æ•°æ›´æ–°åˆ°åˆå¹¶æ–‡ç« åˆ—è¡¨
                for art in filtered:
                    if "score" in art:
                        for m in merged_articles:
                            if normalize_string(m.get("title","")) == normalize_string(art.get("title","")):
                                m["score"] = art["score"]
                                
                # æ›´æ–°ç¼“å­˜
                update_cache_with_scores(filtered)

        # 6) ä¸ºæ²¡æœ‰åˆ†æ•°çš„æ–‡ç« æ·»åŠ é»˜è®¤åˆ†æ•°
        for art in merged_articles:
            if "score" not in art:
                art["score"] = 0

        # 7) è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼ˆå«é«˜å½±å“åŠ›æ‚å¿—åŠ åˆ†ï¼‰
        for art in merged_articles:
            impact_bonus = journal_impact_bonus(art)
            art["final_score"] = art.get("score", 0) + impact_bonus

        # 8) æ’åºè·å–Top10
        top10 = sorted(merged_articles, key=lambda x: x.get("final_score", 0), reverse=True)[:10]

        # 9) åˆ›å»ºGitHub Issue
        today    = datetime.now().strftime("%Y-%m-%d")
        week_ago = (datetime.now() - timedelta(days=RECENT_SEARCH_DAYS)).strftime("%Y-%m-%d")
        title = f"{RESEARCH_AREA}ç ”ç©¶å‘¨æŠ¥ ({week_ago} è‡³ {today})"
        body  = f"## {week_ago} â€“ {today} æœ€ç›¸å…³ Top10ï¼š\n\n"
        
        for art in top10:
            is_high_impact = journal_impact_bonus(art) > 0
            body += (
                f"### {art['title']}\n"
                f"- æ‚å¿—: {art.get('journal','æœªçŸ¥')}{' ğŸŒŸ' if is_high_impact else ''}\n"
                f"- å‘è¡¨æ—¥æœŸ: {art.get('pub_date','æœªçŸ¥')}\n"
                f"- ç›¸å…³æ€§åˆ†æ•°: {art.get('score',0)}/100\n"
                f"- DOI: {art.get('doi','æ— ')}\n"
                f"- é“¾æ¥: {art.get('link','')}\n\n"
            )
        
        success = create_github_issue(title, body)
        logger.info("Issue å·²åˆ›å»º" if success else "Issue åˆ›å»ºå¤±è´¥")

    except Exception as e:
        logger.error(f"ä¸»æµç¨‹æ‰§è¡Œå‡ºé”™: {str(e)}", exc_info=True)
        error_title = f"{RESEARCH_AREA}ç ”ç©¶å‘¨æŠ¥æ‰§è¡Œå‡ºé”™ - {datetime.now().strftime('%Y-%m-%d')}"
        error_body  = f"## æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯\n\n```\n{str(e)}\n```\n\nè¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
        create_github_issue(error_title, error_body)

if __name__ == "__main__":
    main()